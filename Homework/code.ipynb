{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MSPacman.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJnK6TmFz4F-"
      },
      "source": [
        "from typing import Tuple, List, Optional, Dict\n",
        "import numpy as np\n",
        "import dataclasses"
      ],
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNccPvEjt3eZ"
      },
      "source": [
        "#data structure with the information about transitions\n",
        "@dataclasses.dataclass\n",
        "class Transition:\n",
        "    state: Tuple[int, int]\n",
        "    action: str\n",
        "    next_state: Tuple[int, int]\n",
        "    reward: float\n",
        "    termination: bool\n",
        "\n",
        "class PacEnv:\n",
        "    _states: np.array #two-dimensional grid world\n",
        "    _rewards: np.array #depend on ghosts met and points collected\n",
        "    _action_semantics: List[str] #readable representation of the actions\n",
        "    _actions: Dict[str, np.array] #for transitions in the state space\n",
        "    _init_state: Tuple[int, int] #fixed state in the state space\n",
        "    _current_state: Tuple[int, int] #current position of the agent\n",
        "    _ghosts: Optional[List[Tuple[int, int]]] #to be set by user\n",
        "    _transition_probabilities: np.array #encorage the exploration\n",
        "\n",
        "\n",
        "    #parameters of the environment which may be set by user\n",
        "    def __init__(self,\n",
        "                rows: int,\n",
        "                cols: int,\n",
        "                step_cost: float,\n",
        "                lifes: Optional[int],\n",
        "                ghosts: Optional[List[Tuple[int, int]]] = None,\n",
        "                walls: Optional[List[Tuple[int, int]]] = None) -> None:\n",
        "        #the state space created\n",
        "        self._states = np.array(np.zeros((rows, cols)), dtype=str)\n",
        "\n",
        "        walls = [] if walls is None else walls\n",
        "        \n",
        "        #\"p\" stands for \"points\" which should be eaten by PacMan in order to reach his goal\n",
        "        #initially every sell which is not the wall, the ghost or the agent includes point to be eaten\n",
        "        for r in range(0, self._states.shape[0]):\n",
        "            for c in range(0, self._states.shape[1]):\n",
        "                self._states[r, c] = 'p'\n",
        "\n",
        "        #assignings of the walls\n",
        "        for r, c in walls:\n",
        "            self._states[r, c] = 1\n",
        "\n",
        "        #agent considered to get reward 1 for one eaten point and pay penaltie equal to step cost for transitions in empty cells\n",
        "        self._rewards = (1-step_cost)*np.ones((rows, cols))\n",
        "\n",
        "        #reward = -100 if the agent is killed by ghost\n",
        "        for r, c in ghosts:\n",
        "            self._rewards[r, c] = -100\n",
        "\n",
        "        #initializing\n",
        "        self._action_semantics = ['up', 'left', 'down', 'right']\n",
        "        self._actions = np.array([[-1, 0], [0, -1], [1, 0], [0, 1]])\n",
        "        self._init_state = (3, 2)\n",
        "        self._current_state = self._init_state\n",
        "        self._ghosts = ghosts\n",
        "        self._step_cost = step_cost\n",
        "        self._lifes = lifes\n",
        "        self._copy_lifes = lifes\n",
        "\n",
        "        #instead of chosen up/down actions agent could be replaced left/right with the probability = 0.1 and vice versa\n",
        "        self._transition_probabilities = np.array([0.1, 0.8, 0.1])\n",
        "\n",
        "    #to get list of actions  \n",
        "    @property\n",
        "    def actions(self) -> List[str]:\n",
        "        return self._action_semantics\n",
        "\n",
        "    #to get current state\n",
        "    @property\n",
        "    def current_state(self) -> Tuple[int, int]:\n",
        "        return self._current_state\n",
        "\n",
        "    #if agent have already eaten all of the points it will get reward = 100\n",
        "    @property\n",
        "    def reward(self) -> float:\n",
        "        r, c = self._current_state\n",
        "        return self._rewards[r, c] if self.termination == False else 100\n",
        "\n",
        "    #checks the state of the environment: if all of the points are eaten, then goal is reached -> termination\n",
        "    @property\n",
        "    def termination(self) -> bool:\n",
        "        grid = np.array(self._states, dtype = str)\n",
        "        l = list(grid)\n",
        "        goal_flag = True\n",
        "        for i in l:\n",
        "          if 'p' in set(i):\n",
        "              goal_flag = False\n",
        "        return goal_flag\n",
        "\n",
        "    #represents the agent and the ghosts on the instance of the environment\n",
        "    def render(self) -> None:\n",
        "        grid = np.array(self._states, dtype = str)\n",
        "        r, c = self._current_state\n",
        "        grid[r, c] = 'x'\n",
        "\n",
        "        for r, c in self._ghosts:\n",
        "            grid[r, c] = 'g'\n",
        "\n",
        "        print(grid)\n",
        "\n",
        "    #manipulations with the indexes to provide correct transitions of the agent inside the environmental boundaries\n",
        "    #structure of the return of that function provides a lack of possibility to go through walls\n",
        "    def _transition(self, state: Tuple[int, int], a: np.array) -> Tuple[int, int]:\n",
        "        n_actions = len(self._actions)\n",
        "        a = self._actions[a + n_actions if a < 0 else a % n_actions]\n",
        "        new_r = max(0, min(self._states.shape[0] - 1, state[0] + a[0]))\n",
        "        new_c = max(0, min(self._states.shape[1] - 1, state[1] + a[1]))\n",
        "        return (new_r, new_c) if (self._states[new_r, new_c] == 0. or self._states[new_r, new_c] == 'p') else state\n",
        "\n",
        "    #the main transitional mechanism\n",
        "    def step(self, action: str) -> Transition:\n",
        "        a_idx = self._action_semantics.index(action) #get an index given semantics of action\n",
        "\n",
        "        rnd = np.random.rand()\n",
        "        chosen_action = a_idx + np.random.choice([-1, 0, 1], p=self._transition_probabilities) #exploitation/exploration trade-off\n",
        "        prev_state = self._current_state \n",
        "        r, c = self._current_state\n",
        "        #The PacMan game's agent usually has three lifes (may be set by user in my implementation) each of which may be lost by ghost\n",
        "        if self._rewards[r, c] == -100:\n",
        "            self._lifes -= 1\n",
        "        #step itself\n",
        "        self._current_state = self._transition(self._current_state, chosen_action)\n",
        "        #once the cell has been visited it has no points to be eaten anymore\n",
        "        self._states[r, c] = 0.\n",
        "        #in the future in such cells agent will not get a reward for eating a points, only step cost for transition\n",
        "        self._rewards[r, c] = -self._step_cost\n",
        "        #the result of the transition\n",
        "        return Transition(state = prev_state,\n",
        "                          action = action,\n",
        "                          next_state = self._current_state,\n",
        "                          reward = self.reward,\n",
        "                          termination = self.termination)\n",
        "    \n",
        "    #there are two modes for reseting:\n",
        "    #1)if the agent lost all its lifes then: agent in the initial state, all of the points aren't eaten, agent has all lifes again\n",
        "    #2)if the agent is still having lifes after reset it appears to be in the initial state, but eaten previously points are still eaten \n",
        "    def reset(self) -> None:\n",
        "        if self._lifes <= 0:\n",
        "          self._current_state = self._init_state\n",
        "          for r in range(0, self._states.shape[0]):\n",
        "              for c in range(0, self._states.shape[1]):\n",
        "                  if (self._states[r, c] == \"0.0\") or (self._states[r, c] == 'x'):\n",
        "                      self._states[r, c] = 'p'\n",
        "          self._lifes = self._copy_lifes\n",
        "        else:\n",
        "          self._current_state = self._init_state \n",
        "\n",
        "    #these two functions are used for the construction of Q-table\n",
        "    def state_space_size(self) -> int:\n",
        "        return np.prod(list(self._states.shape))\n",
        "    \n",
        "    def action_space_size(self) -> int:\n",
        "        return len(self._actions)"
      ],
      "execution_count": 325,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNK-OzCkzCwd",
        "outputId": "ac78b8e6-3f8e-4a40-94a1-ac8f7f339ad5"
      },
      "source": [
        "#initial situation\n",
        "my_env = PacEnv(rows=6, cols=6, step_cost = 0.04, lifes = 0, ghosts=[(4, 0), (1, 4)], walls=[(1,1), (2,1), (1,2), (4,4), (3,4), (4,3), (2,4)])\n",
        "my_env.render()\n",
        "print(my_env.current_state)\n",
        "print(my_env.reward)\n",
        "print(my_env.termination)\n",
        "print(my_env.actions)\n"
      ],
      "execution_count": 319,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['p' 'p' 'p' 'p' 'p' 'p']\n",
            " ['p' '1' '1' 'p' 'g' 'p']\n",
            " ['p' '1' 'p' 'p' '1' 'p']\n",
            " ['p' 'p' 'x' 'p' '1' 'p']\n",
            " ['g' 'p' 'p' '1' '1' 'p']\n",
            " ['p' 'p' 'p' 'p' 'p' 'p']]\n",
            "(3, 2)\n",
            "0.96\n",
            "False\n",
            "['up', 'left', 'down', 'right']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJrikh3rzOYO",
        "outputId": "f4e3af70-1cde-4bd9-cc86-889e2df4b01a"
      },
      "source": [
        "#test where I created an instance of the environment and executed a sequence of the 5th random actions\n",
        "my_env.reset()\n",
        "rnd = np.random.rand()\n",
        "for i in range(0, 5):\n",
        "    a = np.random.choice([\"up\", \"down\", \"right\", \"left\"], p=[0.25, 0.25, 0.25, 0.25])   \n",
        "    print(a)\n",
        "    print(my_env.step(a))\n",
        "    my_env.render()\n",
        "my_env.reset()\n",
        "my_env.render()"
      ],
      "execution_count": 320,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "down\n",
            "Transition(state=(3, 2), action='down', next_state=(4, 2), reward=0.96, termination=False)\n",
            "[['p' 'p' 'p' 'p' 'p' 'p']\n",
            " ['p' '1' '1' 'p' 'g' 'p']\n",
            " ['p' '1' 'p' 'p' '1' 'p']\n",
            " ['p' 'p' '0.0' 'p' '1' 'p']\n",
            " ['g' 'p' 'x' '1' '1' 'p']\n",
            " ['p' 'p' 'p' 'p' 'p' 'p']]\n",
            "left\n",
            "Transition(state=(4, 2), action='left', next_state=(4, 1), reward=0.96, termination=False)\n",
            "[['p' 'p' 'p' 'p' 'p' 'p']\n",
            " ['p' '1' '1' 'p' 'g' 'p']\n",
            " ['p' '1' 'p' 'p' '1' 'p']\n",
            " ['p' 'p' '0.0' 'p' '1' 'p']\n",
            " ['g' 'x' '0.0' '1' '1' 'p']\n",
            " ['p' 'p' 'p' 'p' 'p' 'p']]\n",
            "left\n",
            "Transition(state=(4, 1), action='left', next_state=(4, 0), reward=-100.0, termination=False)\n",
            "[['p' 'p' 'p' 'p' 'p' 'p']\n",
            " ['p' '1' '1' 'p' 'g' 'p']\n",
            " ['p' '1' 'p' 'p' '1' 'p']\n",
            " ['p' 'p' '0.0' 'p' '1' 'p']\n",
            " ['g' '0.0' '0.0' '1' '1' 'p']\n",
            " ['p' 'p' 'p' 'p' 'p' 'p']]\n",
            "right\n",
            "Transition(state=(4, 0), action='right', next_state=(4, 0), reward=-0.04, termination=False)\n",
            "[['p' 'p' 'p' 'p' 'p' 'p']\n",
            " ['p' '1' '1' 'p' 'g' 'p']\n",
            " ['p' '1' 'p' 'p' '1' 'p']\n",
            " ['p' 'p' '0.0' 'p' '1' 'p']\n",
            " ['g' '0.0' '0.0' '1' '1' 'p']\n",
            " ['p' 'p' 'p' 'p' 'p' 'p']]\n",
            "up\n",
            "Transition(state=(4, 0), action='up', next_state=(3, 0), reward=0.96, termination=False)\n",
            "[['p' 'p' 'p' 'p' 'p' 'p']\n",
            " ['p' '1' '1' 'p' 'g' 'p']\n",
            " ['p' '1' 'p' 'p' '1' 'p']\n",
            " ['x' 'p' '0.0' 'p' '1' 'p']\n",
            " ['g' '0.0' '0.0' '1' '1' 'p']\n",
            " ['p' 'p' 'p' 'p' 'p' 'p']]\n",
            "[['p' 'p' 'p' 'p' 'p' 'p']\n",
            " ['p' '1' '1' 'p' 'g' 'p']\n",
            " ['p' '1' 'p' 'p' '1' 'p']\n",
            " ['p' 'p' 'x' 'p' '1' 'p']\n",
            " ['g' 'p' 'p' '1' '1' 'p']\n",
            " ['p' 'p' 'p' 'p' 'p' 'p']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHRg5celOuAN"
      },
      "source": [
        "## **Policy iteration by Q-learning algorithm**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "vQY3zDNw4J4g",
        "outputId": "2e8233b8-7ec4-417e-f3b1-aced0cffa692"
      },
      "source": [
        "#list of the parameters and constants\n",
        "gamma = 1\n",
        "epsilon = 0.1\n",
        "alpha = 0.01\n",
        "state_space_size = my_env.state_space_size()\n",
        "action_space_size = my_env.action_space_size()\n",
        "actions = my_env.actions\n",
        "n_episodes = 1000\n",
        "episode_size = 200\n",
        "visited = {}\n",
        "\n",
        "#action-value function to be updated\n",
        "q: Dict[int, List[float]] = {i: [0. for a in actions] for i in range(state_space_size)}\n",
        "\n",
        "#dict of actions which are taken by epsilon-greedy policy\n",
        "def policy(epsilon: float, actions: List[str], episode: int) -> Dict[int, str]:\n",
        "    if episode == n_episodes - 1:\n",
        "        epsilon = 0.0\n",
        "    pi = {i: actions[np.argmax(q[i])] if np.random.rand() >= epsilon \n",
        "          else np.random.choice(actions, p=[0.25, 0.25, 0.25, 0.25]) for i in q.keys()}\n",
        "    return pi\n",
        "\n",
        "#implementational cycle\n",
        "for episode in range(n_episodes):\n",
        "    if episode%100==0:\n",
        "      print(f'Episode {episode}')\n",
        "\n",
        "    #get a set of actions for this episode    \n",
        "    my_env.reset()\n",
        "    pi = policy(epsilon, actions, episode)\n",
        "    \n",
        "    for stage in range(episode_size):\n",
        "        #check for visited states\n",
        "        s = my_env.current_state\n",
        "        if s in visited:\n",
        "            s = visited[s]\n",
        "        else:\n",
        "            visited[s] = len(visited)\n",
        "            s = visited[s]\n",
        "        #step\n",
        "        action = pi[s]\n",
        "        transition = my_env.step(action)\n",
        "        a_idx = actions.index(action)\n",
        "\n",
        "        next_s = transition.next_state\n",
        "        if next_s in visited:\n",
        "            next_s = visited[next_s]\n",
        "        else:\n",
        "            visited[next_s] = len(visited)\n",
        "            next_s = visited[next_s]\n",
        "        #updating\n",
        "        if transition.termination:\n",
        "            q[s][a_idx] += alpha*(transition.reward - q[s][a_idx])\n",
        "            #interrupting of the cycle once the termination occasioned\n",
        "            break\n",
        "        else:\n",
        "            q[s][a_idx] += + alpha*(transition.reward + gamma * np.max(q[next_state]) - q[s][a_idx])\n"
      ],
      "execution_count": 327,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Episode 0\n",
            "Episode 100\n",
            "Episode 200\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-327-3685351a713f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpi\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0mtransition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmy_env\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m         \u001b[0ma_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-316-a323a51561c0>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    106\u001b[0m                           \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m                           \u001b[0mnext_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m                           \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m                           termination = self.termination)\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-316-a323a51561c0>\u001b[0m in \u001b[0;36mreward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtermination\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-316-a323a51561c0>\u001b[0m in \u001b[0;36mtermination\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mgoal_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m           \u001b[0;32mif\u001b[0m \u001b[0;34m'p'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m               \u001b[0mgoal_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgoal_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3wXvUao4m3m1"
      },
      "source": [
        ""
      ],
      "execution_count": 298,
      "outputs": []
    }
  ]
}